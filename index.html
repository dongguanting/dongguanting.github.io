
<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-EWB73JXBN5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-EWB73JXBN5');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="description" content="Homepage of KABI">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
    <title>Guanting Dong's Homepage|Ëë£ÂÜ†ÈúÜÁöÑ‰∏™‰∫∫‰∏ªÈ°µ</title>
    <style>
        * {
            box-sizing: border-box;
        }
        
        body {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
            font-family: Arial, sans-serif;
            line-height: 1.6;
        }
        
        #layout-content {
            width: 100%;
            padding: 30px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin: 0 auto;
        }
        
        #toptitle {
            margin-bottom: 30px;
            text-align: left;
        }
        
        #toptitle h1 {
            color: #333;
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        .profile-section {
            display: flex;
            align-items: flex-start;
            gap: 30px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        
        .profile-info {
            flex: 1;
            min-width: 300px;
        }
        
        .profile-photo {
            flex-shrink: 0;
            text-align: center;
        }
        
        .profile-photo img {
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        
        h2 {
            color: #2c3e50;
            font-size: 1.8em;
            margin-top: 40px;
            margin-bottom: 20px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        
        h3 {
            color: #34495e;
            font-size: 1.4em;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        .scroll-container {
            width: 100%;
            height: 300px;
            overflow-y: auto;
            border: 1px solid #ddd;
            border-radius: 6px;
            padding: 15px;
            background-color: #fafafa;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.05);
        }
        
        ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }
        
        li {
            margin-bottom: 15px;
            padding: 10px;
            background-color: white;
            border-radius: 4px;
            border-left: 4px solid #3498db;
        }
        
        .experiences ul li {
            background-color: #f8f9fa;
            border-left: 4px solid #28a745;
        }
        
        .publications ul li {
            background-color: #fff8f0;
            border-left: 4px solid #ff8c00;
        }
        
        .honors ul li {
            background-color: #f0f8ff;
            border-left: 4px solid #4169e1;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
            transition: color 0.3s ease;
        }
        
        a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        
        .section-divider {
            height: 2px;
            background: linear-gradient(to right, #3498db, transparent);
            margin: 40px 0;
            border: none;
        }
        
        #footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            text-align: center;
            color: #666;
            font-size: 0.9em;
        }
        
        .stats {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .stat-item {
            padding: 10px;
            background-color: #f8f9fa;
            border-radius: 4px;
            min-width: 120px;
            text-align: center;
        }
        
        /* ÂìçÂ∫îÂºèËÆæËÆ° */
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            #layout-content {
                padding: 20px;
            }
            
            .profile-section {
                flex-direction: column;
                text-align: center;
            }
            
            #toptitle h1 {
                font-size: 2em;
            }
            
            h2 {
                font-size: 1.5em;
            }
        }
    </style>
</head>

<body>
    <div id="layout-content" style="margin-top: 25px;">

        <div id="toptitle">
            <h1>Guanting Dong (Ëë£ÂÜ†ÈúÜ)&nbsp;</h1>
        </div>

        <h3>Ph.D. Student</h3>  
        <p>
            Dept. of Artificial Intelligence <br>
            Renmin University of China (RUC) <br>
            Beijing, China, 100084. <br>
            WeChat: dongguanting990611 <br>
            Email: <a href="mailto:dongguanting@ruc.edu.cn">dongguanting@ruc.edu.cn</a> <br>
            <a href="https://github.com/dongguanting">[Github]</a>
            <a href="https://scholar.google.com/citations?user=amozZDkAAAAJ&hl=zh-CN&oi=ao">[Google Scholar]</a>
            <a href="https://www.semanticscholar.org/author/Guanting-Dong/51490462?sort=influence">[Semantic Scholar]</a> 
            <a href="https://dblp.org/pid/227/7667.html">[DBLP]</a> 
            <a href="https://twitter.com/kakakbibibi">[Twitter]</a> 
            <a href="https://www.zhihu.com/people/kakakakabi/posts">[Áü•‰πé]</a> 
        </p>
    </td>

    <td><img src="./files/photo.jpg" border="0" width="120"></td>
</tr></tbody></table>

<h2>About</h2>
    <p>I am a first-year Ph.D. student at the <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, Renmin University of China, fortunate to be co-advised by <a href="http://playbigdata.ruc.edu.cn/dou/">Prof. Zhicheng Dou</a> and <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ&hl=zh-CN">Prof. Jirong Wen</a>. Previously, I received M.Eng (2024) and B.Eng (2021) degrees in Information and Communication Engineering from Beijing University of Posts and Telecommunications(BUPT), advised by <a href="https://pris-nlp.github.io/en/author/weiran-xu/">Prof. Weiran Xu</a>.</p>
    
    <p> Currently, my research interests focus on 
    <ul>
        <li>Deep Search Agent</li>
        <li>Alignment for Large Language Models</li>
        <li>Large Language Models Reasoning</li>
    </ul>
    </p>
    <p> My long-term goal is to explore an automated, scalable, and safe way that fosters exceptional intelligence to achieve AGI.</p>

<h2>News</h2>
<div class="scroll-container">
    <ul>
        <li><strong>[2024-04]</strong> We propose <a href='https://arxiv.org/abs/2504.21776'>üîßüåüTool-Star</a >, a LLM-brained multi-tool reasoner via reinforcement learning! Check out our project <a href='https://github.com/dongguanting/Tool-Star'>here!</a ></li>
        <li><strong>[2024-05]</strong> Four papers have been accepted by ACL 2024! Looking forward to seeing you in Vienna! </li>
        <li><strong>[2024-04]</strong> We introduce <a href='https://arxiv.org/abs/2504.21776'>üåê WebThinker</a >, a powerful open-sourced deep research agent! Feel free to check out our <a href='https://foremost-beechnut-8ed.notion.site/WebThinker-Empowering-Large-Reasoning-Models-with-Deep-Research-Capability-d13158a27d924a4b9df7f9ab94066b64'>Demo</a >!</li>
        <li><strong>[2024-02]</strong> Our modular toolkit <a href='https://arxiv.org/pdf/2405.13576'>‚ö°FlashRAG</a > currently supports a range of multimodal retrievers and generators, please check it out!</li>
        <li><strong>[2024-01]</strong> <a href='https://arxiv.org/pdf/2406.18676'>DPA-RAG</a > has been accepted by WWW 2025, which is designed to align diverse preferences within RAG systems.</li>
        <li><strong>[2025-01]</strong> We release <a href='https://arxiv.org/pdf/2501.05366'>Search-o1</a >, leveraging agentic search mechanism to enhance o1-like large reasoning models!</li>
        <li><strong>[2025-01]</strong> Two papers have been accepted by ICLR 2025! <a href='https://arxiv.org/pdf/2406.13542'>AUTOIF</a > is the secret behind <img src="./files/qwen_logo.webp" style="width: 1em;" /> Qwen's instruction-following alignment.</li>
        <li><strong>[2024-12]</strong> Honored to be a contributor to the Qwen2.5 <img src="./files/qwen_logo.webp" style="width: 1em;" />, a series of LLMs designed to meet diverse needs!</li>
        <li><strong>[2024-12]</strong> <a href='https://arxiv.org/abs/2410.09584'>FollowRAG</a > has been accepted by AAAI 2025, which is dedicated to developing a reliable instruction-following RAG system.</li>
        <li><strong>[2024-09]</strong> Glad to be a Ph.D. student at GSAI, Renmin University of China.</li>
        <!-- ËøôÈáåÂèØ‰ª•Ê∑ªÂä†Êõ¥Â§öÊù°ÁõÆ -->
    </ul>
</div>



<h2>Experiences</h2>
	<strong>Academia</strong>
	<ul>
    <li>[2024.9 - Now]    <img style="width: 1em;"src="./files/ruc.webp"> Ph.D. at Renmin University of China</li>
	<li>[2021.9 - 2024.6]<img style="width: 1em;"src="./files/bupt.webp"> M.S. at Beijing University of Posts and Telecommunications</li>
		<li style="margin-left: 40px;">[2018.7 - 2018.8] <img style="width: 1em;"src="./files/oxford.webp"> Summer Exchange Internship at University of Oxford, UK</li>
	<li>[2017.9 - 2021.6] <img style="width: 1em;"src="./files/bupt.webp"> B.S. at Beijing University of Posts and Telecommunications</li>
    
</ul>


 <strong>Industry</strong>
	<ul>
	<li>[2023.6 - 2024.8] <img style="width: 2.4em;"src="./files/alibaba.png"> Alibaba, Qwen Team, Research Intern on Alignment & Reasoning of Large Language Models</li>
	<li>[2022.9 - 2023.5] <img style="width: 2.4em;"src="./files/meituan.png"> Meituan, NLP Center, Research Intern on Knowledge Augmented Generation</li>
</ul>

	
	
<h2>Selected Publications</h2>

(* denotes equal contributions)<br>

<h3>As the Core Author.</h3>

	
<strong>-2025-</strong>


<ul><li><p>RAG-Critic: Leveraging Automated Critic-Guided Agentic Workflow for Retrieval Augmented Generation<br />
    <strong>Guanting Dong</strong>, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen<br />
    ACL 2025 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/pdf/2412.14835'>[paper]</a >
	<a href='https://github.com/dongguanting/RAG-Critic'>[code]</a><br>
	
</li>
</ul>

	

<ul><li><p>Progressive Multimodal Reasoning via Active Retrieval<br />
    <strong>Guanting Dong</strong>, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen<br />
    ACL 2025 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/pdf/2412.14835'>[paper]</a >
	<a href='https://zhuanlan.zhihu.com/p/15638015036'>[blog]</a><br>
	
</li>
</ul>


<ul><li><p>We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?<br />
    Runqi Qiao, Qiuna Tan, <strong>Guanting Dong‚àó</strong>, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, Honggang Zhang<br />
    ACL 2025 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/pdf/2407.01284'>[paper]</a >
	<a href='https://we-math.github.io/'>[homepage]</a >
	<a href='https://github.com/We-Math/We-Math'>[code]</a >
	<a href='https://huggingface.co/datasets/We-Math/We-Math'>[dataset]</a >
    <a href='https://mp.weixin.qq.com/s/uU1lZV0Ymj31cmZryhffyQ'>[blog]</a><br>
</li>
</ul>
	
	
<ul><li><p>Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation<br />
    <strong>Guanting Dong</strong>, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen<br />
       ACL 2025 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/pdf/2406.18676'>[paper]</a >
	<a href='https://github.com/dongguanting/DPA-RAG'>[code]</a >
    <a href='https://mp.weixin.qq.com/s/GddtwVZBWlGVpugVHTjUOQ'>[blog]</a><br>
</li>
</ul>


	

<ul><li><p>Toward General Instruction-Following Alignment for Retrieval-Augmented Generation<br />
    <strong>Guanting Dong</strong>, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, Ji-Rong Wen<br />
    AAAI 2025 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/abs/2410.09584'>[paper]</a >
    <a href='https://followrag.github.io/'>[homepage]</a >
	<a href='https://github.com/dongguanting/FollowRAG'>[code]</a >
    <a href='https://huggingface.co/datasets/dongguanting/VIF-RAG-QA-110K'>[dataset]</a>
	<a href='https://zhuanlan.zhihu.com/p/15652951301'>[blog]</a><br>
</li>
</ul>

	
<ul><li><p>Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models<br />
    <strong>Guanting Dong</strong>, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou<br />
    ICLR 2025 <span style="color: #A40000;">(Spotlight, Top 5% of all papers)</span>.
    <a href='https://arxiv.org/pdf/2406.13542'>[paper]</a >
	<a href='https://github.com/QwenLM/AutoIF'>[code]</a >
    <a href='https://zhuanlan.zhihu.com/p/707012952'>[blog]</a><br>
</li>
</ul>

	
<ul><li><p>CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery<br />
    Xiaoshuai Song, Muxi Diao, <strong>Guanting Dong</strong>, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma GongQue, Jianing Yu, Qiuna Tan, Weiran Xu<br />
    ICLR 2025
    <a href='https://arxiv.org/pdf/2406.08587'>[paper]</a >
	<a href='https://csbench.github.io/'>[homepage]</a >
	<a href='https://github.com/csbench/csbench'>[code]</a >
	<a href='https://huggingface.co/datasets/CS-Bench/CS-Bench'>[dataset]</a >
    <a href='https://mp.weixin.qq.com/s/9DDSbKJt3rYisJi95fJ46w'>[blog]</a><br>

</li>
</ul>


	

	
	

<strong>-2024-</strong>
	



	
<ul><li><p>How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition<br />
    <strong>Guanting Dong</strong>, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou<br />
    ACL 2024 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/pdf/2310.05492.pdf'>[paper]</a>
	<a href='https://mp.weixin.qq.com/s/3RIBzuVlK0qHbO_Q04s-cw'>[blog]</a><br>
</li>
</ul>
	



<ul><li><p>Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task<br />
Jinxu Zhao, <strong>Guanting Dong‚àó</strong>, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song, Daichi Guo, Weiran Xu<br />
ICASSP 2024 <span style="color: #A40000;">(CCF-B)</span>. 
<a href='https://arxiv.org/pdf/2402.14494.pdf'>[paper]</a>
</li>
</ul>



	
<strong>-2023-</strong>



	
<ul><li><p>DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task<br />
<strong>Guanting Dong</strong>, Tingfeng Hui, Zhuoma GongQue, Jinxu Zhao, Daichi Guo, Gang Zhao, Keqing He, Weiran Xu<br />
Findings of EMNLP 2023 (Shot Paper). 
<a href='https://aclanthology.org/2023.findings-emnlp.705.pdf'>[paper]</a>
<a href='https://github.com/dongguanting/Demo-NSF'>[code]</a>
<a href='https://mp.weixin.qq.com/s/zkdUkybrTafPQR9wpVmi3w'>[blog]</a><br>
	
</li>
</ul>


<ul><li><p>Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking<br />
Yuxiang Wu, <strong>Guanting Dong*</strong>, Weiran Xu<br />
Findings of EMNLP 2023 (Short Paper). 
<a href='https://aclanthology.org/2023.findings-emnlp.741.pdf'>[paper]</a>
<a href='https://github.com/ToLightUpTheSky/ParsingDST'>[code]</a> <br>
</li>
</ul>

<ul><li><p>A Multi-Task Semantic Decomposition Framework with Task-specific Pre-training for Few-Shot NER<br />
<strong>Guanting Dong</strong>, Zechen Wang, Jinxu Zhao, Gang Zhao, Daichi Guo, Dayuan Fu, Tingfeng Hui, Chen Zeng, Keqing He, Xuefeng Li, Liwen Wang, Xinyue Cui, Weiran Xu<br />
CIKM 2023 (Oral) <span style="color: #A40000;">(CCF-B)</span>.
<a href='https://arxiv.org/pdf/2308.14533.pdf'>[paper]</a>
<a href='https://github.com/dongguanting/MSDP-Fewshot-NER'>[code]</a> <br>
</li>
</ul>


<ul><li><p>Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA<br />
<strong>Guanting Dong</strong>, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, Weiran Xu<br />
CIKM 2023 (Short Paper). 
<a href='https://arxiv.org/pdf/2308.14436.pdf'>[paper]</a>
<a href='https://github.com/dongguanting/SKP-for-KBQA'>[code]</a> <br>
</li>
</ul>

<ul><li><p>Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task<br />
<strong>Guanting Dong</strong>,  Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wang, Boqi Feng, Yueyan Qiu, Zhuoma Gongque, Keqing He, Zechen Wang, Weiran Xu<br />
NLPCC 2023 (Oral) <span style="color: #A40000;">(CCF-C)</span>.
<a href='https://arxiv.org/pdf/2310.06504.pdf'>[paper]</a>
<a href='https://github.com/dongguanting/Noise-Slot-Filling-LLM'>[code]</a> <br>
</li>
</ul>


<ul><li><p>Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting<br />
Xuefeng Li, Liwen Wang, <strong>Guanting Dong*</strong>, Keqing He, Jinzheng Zhao, Hao Lei, Jiachi Liu, Weiran Xu<br />
Findings of ACL 2023 (Short Paper). 
<a href='https://aclanthology.org/2023.findings-acl.52.pdf'>[paper]</a>
<a href='https://github.com/LiXuefeng2020ai/GZPL'>[code]</a> <br>
</li>
</ul>


<ul><li><p>A Prototypical Semantic Decoupling Method via Joint Contrastive Learning for Few-Shot Named Entity Recognition<br />
<strong>Guanting Dong</strong>, Zechen Wang, Liwen Wang, Daichi Guo, Dayuan Fu, Yuxiang Wu, Chen Zeng, Xuefeng Li, Tingfeng Hui, Keqing He, Xinyue Cui, Qixiang Gao, Weiran Xu<br />
ICASSP 2023 <span style="color: #A40000;">(CCF-B)</span>. 
<a href='https://ieeexplore.ieee.org/abstract/document/10095149'>[paper]</a>
</li>
</ul>

<ul><li><p>Revisit Out-Of-Vocabulary Problem For Slot Filling: A Unified Contrastive Framework With Multi-Level Data Augmentations<br />
Daichi Guo, <strong>Guanting Dong*</strong>, Dayuan Fu, Yuxiang Wu, Chen Zeng, Tingfeng Hui, Liwen Wang, Xuefeng Li, Zechen Wang, Keqing He, Xinyue Cui, Weiran Xu<br />
ICASSP 2023 <span style="color: #A40000;">(CCF-B)</span>. 
<a href='https://ieeexplore.ieee.org/abstract/document/10094766/'>[paper]</a>
</li>
</ul>

<strong>-2022-</strong>

<ul><li><p>Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking<br />
Gao Qixiang, <strong>Guanting Dong*</strong>, Yutao Mou, Liwen Wang, Chen Zeng, Daichi Guo, Mingyang Sun, Weiran Xu<br />
EMNLP 2022 (Oral) (Short Paper). 
<a href='https://aclanthology.org/2022.emnlp-main.157.pdf'>[paper]</a>
</li>
</ul>


<ul><li><p>Entity-level Interaction via Heterogeneous Graph for Multimodal Named Entity Recognition<br />
Gang Zhao, <strong>Guanting Dong</strong>, Yidong Shi, Haolong Yan, Weiran Xu, Si Li<br />
Findings of EMNLP 2022 (Short Paper). 
<a href='https://aclanthology.org/2022.findings-emnlp.473.pdf'>[paper]</a>
<a href='https://github.com/GangZhao98/GEI'>[code]</a> <br>
</li>
</ul>


<ul><li><p>PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling<br />
<strong>Guanting Dong</strong>, Daichi Guo, Liwen Wang, Xuefeng Li, Zechen Wang, Chen Zeng, Keqing He, Jinzheng Zhao, Hao Lei, Xinyue Cui, Yi Huang, Junlan Feng, Weiran Xu<br />
COLING 2022 (Short Paper). 
<a href='https://aclanthology.org/2022.coling-1.473.pdf'>[paper]</a>
</li>
</ul>


<hr>
	

<h3>As a Co-author.</h3>



<strong>-2025-</strong>



<ul><li><p>Hierarchical Document Refinement for Long-context Retrieval-augmented Generation<br />
  Jiajie Jin, Xiaoxi Li, <strong>Guanting Dong</strong>, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou<br />
    ACL 2024 <span style="color: #A40000;">(CCF-A)</span>.
     <a href='https://www.arxiv.org/pdf/2505.10413'>[paper]</a >
     <a href='https://github.com/ignorejjj/LongRefiner'>[code]</a >
 </li>
 </ul>
	
	
	

<ul><li><p>CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation<br />
    Yiruo Cheng, Kelong Mao, Ziliang Zhao,  <strong>Guanting Dong</strong>, Hongjin Qian, Yongkang Wu, Tetsuya Sakai, Ji-Rong Wen, Zhicheng Dou<br />
     Findings of NAACL 2025.
     <a href='https://arxiv.org/pdf/2410.23090'>[paper]</a >
     <a href='https://github.com/Ariya12138/CORAL'>[code]</a >
 </li>
 </ul>
 

<ul><li><p>‚ö°FlashRAG: A Python Toolkit for Efficient RAG Research<br />
	Jiajie Jin, Yutao Zhu, <strong>Guanting Dong</strong>, Yuyao Zhang, Xinyu Yang, Chenghao Zhang, Tong Zhao, Zhao Yang, Zhicheng Dou, Ji-Rong Wen <br />
    WWW 2025 (Resource Track).
  <a href='https://arxiv.org/pdf/2405.13576'>[paper]</a >
	<a href='https://ruc-nlpir.github.io/FlashRAG/#/'>[homepage]</a >
	<a href='https://github.com/RUC-NLPIR/FlashRAG'>[code]</a >
	<a href='https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets'>[Dataset]</a><br>
</li>
</ul>


<ul><li><p>Knowledge Editing on Black-box Large Language Models<br />
    Xiaoshuai Song, Zhengyang Wang, Keqing He, <strong>Guanting Dong</strong>, Jinxu Zhao, Weiran Xu<br />
        WWW 2025 <span style="color: #A40000;">(CCF-A)</span>.
        <a href='https://arxiv.org/pdf/2402.08631.pdf'>[paper]</a>
        <a href='https://github.com/songxiaoshuai/postEdit'>[code]</a> 
        <a href='https://mp.weixin.qq.com/s/KUIAugOzQzcbfPc4jlGeuQ'>[blog]</a><br>
    </li>
    </ul>


<ul><li><p>INSNER: A generative instruction-based prompting method for boosting performance in few-shot NER<br />
    Peiwen Zhao, Chong Feng, Peiguang Li, <strong>Guanting Dong</strong>, Sirui Wang<br />
    Information Processing & Management <span style="color: #A40000;">(SCI-Q1)</span>.
    <a href='https://www.sciencedirect.com/science/article/abs/pii/S0306457324003996'>[paper]</a >
</li>
</ul>

	


<strong>-2024-</strong>

	

<ul><li><p>MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making<br />
   Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, <strong>Guanting Dong</strong>, Bowen Zhou<br />
    EMNLP 2024 <span style="color: #A40000;">(CCF-B)</span>.
    <a href='https://arxiv.org/pdf/2409.16686'>[paper]</a >
</li>
</ul>


<ul><li><p>MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning<br />
Chengpeng Li, Zheng Yuan, Hongyi Yuan, <strong>Guanting Dong</strong>, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou<br />
    ACL 2024 <span style="color: #A40000;">(CCF-A)</span>. 
    <a href='https://arxiv.org/pdf/2310.05506.pdf'>[paper]</a>
	<a href='https://github.com/LHRLAB/ChatKBQA'>[code]</a>
	<a href='https://zhuanlan.zhihu.com/p/663463273'>[blog]</a><br>
</li>
</ul>
	
<ul><li><p>ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models
<br />
Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, <strong>Guanting Dong</strong>, Meina Song, Wei Lin<br />
    Findings of ACL 2024. 
    <a href='https://arxiv.org/pdf/2310.08975'>[paper]</a>
	<a href='https://github.com/OFA-Sys/gsm8k-ScRel'>[code]</a>
	<a href='https://mp.weixin.qq.com/s/rBioXGaCefgOJnrkcSOs3A'>[blog]</a><br>
</li>
</ul>
	
<ul><li><p>DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning<br />
Yejie Wang, Keqing He, <strong>Guanting Dong</strong>, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, Weiran Xu<br />
    ACL 2024 <span style="color: #A40000;">(CCF-A)</span>.
    <a href='https://arxiv.org/pdf/2402.09136.pdf'>[paper]</a>
</li>
</ul>


	
<strong>-2023-</strong>


<ul><li><p>Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT<br />
Xiaoshuai Song, Keqing He, Pei Wang,  <strong>Guanting Dong</strong>, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu<br />
    EMNLP 2024 <span style="color: #A40000;">(CCF-B)</span>.
    <a href='https://aclanthology.org/2023.emnlp-main.636.pdf'>[paper]</a>
	<a href='https://github.com/songxiaoshuai/OOD-Evaluation'>[code]</a> 
</li>
</ul>
	

<ul><li><p>Pay Attention to Implicit Attribute Values: A Multi-modal Generative Framework for AVE Task<br />
Yupeng Zhang, Shensi Wang, Peiguang Li, <strong>Guanting Dong</strong>, Sirui Wang, Yunsen Xian, Zhoujun Li, Hongzhi Zhang<br />
    Findings of ACL 2023 (Short Paper). 
    <a href='https://aclanthology.org/2023.findings-acl.831/'>[paper]</a>
	<a href='https://github.com/G0vi/DEFLATE'>[code]</a> 
	<a href='https://mp.weixin.qq.com/s/KUIAugOzQzcbfPc4jlGeuQ'>[blog]</a><br>
</li>
</ul>

	


<strong>-2022-</strong>
	
<ul><li><p>A Robust Contrastive Alignment Method for Multi-domain Text Classification<br />
    Xuefeng Li, Hao Lei, Liwen Wang, <strong>Guanting Dong</strong>, Jinzheng Zhao, Jiachi Liu, Weiran Xu, Chunyun Zhang<br />
    ICASSP 2022 <span style="color: #A40000;">(CCF-B)</span>. 
    <a href='https://ieeexplore.ieee.org/abstract/document/9747192'>[paper]</a>

</li>
</ul>



	
<h2>Selected Preprints</h2>

(* denotes equal contributions)<br>


<strong>-2025-</strong>

<ul><li><p>Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning<br />
	
	<strong>Guanting Dong</strong>, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen<br />
    Arxiv.
  <a href='https://arxiv.org/abs/2505.16410'>[paper]</a >
	<a href='https://github.com/dongguanting/Tool-Star'>[code]</a >
	<a href='https://mp.weixin.qq.com/s/UNP3P2GEtIuYhT7Z8wIV1g?scene=1'>[blog]</a><br>
	
</li>
</ul>

	
	
<ul><li><p>WebThinker: Empowering Large Reasoning Models with Deep Research Capability<br />
	
	Xiaoxi Li‚àó, Jiajie Jin‚àó, <strong>Guanting Dong‚àó</strong>, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, Zhicheng Dou<br />
    Arxiv.
  <a href='https://arxiv.org/abs/2504.21776'>[paper]</a >
	<a href='https://github.com/RUC-NLPIR/WebThinker'>[code]</a >
	<a href='https://mp.weixin.qq.com/s/lVrTZQLmrJkkG5QYcEZTFA'>[blog]</a><br>
	
</li>
</ul>

	
<ul><li><p>Search-o1: Agentic Search-Enhanced Large Reasoning Models<br />
	Xiaoxi Li, <strong>Guanting Dong</strong>, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou<br />
    Arxiv.
  <a href='https://arxiv.org/pdf/2501.05366'>[paper]</a >
    <a href='https://search-o1.github.io/'>[homepage]</a >
	<a href='https://github.com/sunnynexus/Search-o1'>[code]</a >
	<a href='https://mp.weixin.qq.com/s/gqnGyMM_KYYwDbHyWkIIuw'>[blog]</a><br>
	
</li>
</ul>



</ul>



<strong>-2024-</strong>





<ul><li><p><img src="./files/qwen_logo.webp" style="width: 1em;" /> Qwen2.5 Technical Report<br />

Qwen Team (129 authors including <strong>Guanting Dong</strong>)<br />
    Arxiv.
    <a href='https://arxiv.org/pdf/2412.15115'>[paper]</a >
	<a href='https://github.com/QwenLM/Qwen2.5'>[code]</a >
    <a href='https://huggingface.co/Qwen'>[model]</a >
    <a href='https://modelscope.cn/organization/qwen'>[modelscope]</a >
    <a href='https://qwenlm.github.io/blog/qwen2.5/'>[blog]</a><br>
</li>
</ul>

	

<ul><li><p><img src="./files/qwen_logo.webp" style="width: 1em;" /> Qwen2 Technical Report<br />
    Qwen Team (64 authors including <strong>Guanting Dong</strong>)<br />
	Arxiv.
    <a href='https://arxiv.org/pdf/2407.10671'>[paper]</a >
	<a href='https://github.com/QwenLM/Qwen2'>[code]</a >
    <a href='https://huggingface.co/Qwen'>[model]</a >
    <a href='https://qwenlm.github.io/blog/qwen2/'>[blog]</a><br>
</li>
</ul>



<ul><li><p>DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning<br />
    Chengpeng Li, <strong>Guanting Dong</strong>, Mingfeng Xue, Ru Peng, Xiang Wang, Dayiheng Liu<br />
    Arxiv.
    <a href='https://arxiv.org/pdf/2407.04078'>[paper]</a >
	<a href='https://github.com/ChengpengLi1003/DotaMath'>[code]</a >
	<a href='https://huggingface.co/datasets/dongguanting/DotamathQA'>[dataset]</a >
    <a href='https://mp.weixin.qq.com/s/pH-A9MuDCCDaeOseeogghQ'>[blog]</a><br>
</li>
</ul>

<ul><li><p>Smaller Language Models Are Better Instruction Evolvers<br />
    Tingfeng Hui, Lulu Zhao,  <strong>Guanting Dong</strong>, Yaqi Zhang, Hua Zhou, Sen Su<br />
     Arxiv.
     <a href='https://arxiv.org/pdf/2412.11231'>[paper]</a >
     <a href='https://github.com/HypherX/Evolution-Analysis'>[code]</a >
     <a href='https://zhuanlan.zhihu.com/p/15668033358'>[blog]</a><br>
 </li>
 </ul>


<strong>-2023-</strong>



<ul><li><p>Scaling relationship on learning mathematical reasoning with large language models<br />
    Zheng Yuan, Hongyi Yuan, Chengpeng Li, <strong>Guanting Dong</strong>, Chuanqi Tan, Chang Zhou<br />
    Arxiv. 
    <a href='https://arxiv.org/pdf/2308.01825.pdf'>[paper]</a>
	<a href='https://github.com/OFA-Sys/gsm8k-ScRel'>[code]</a>
	<a href='https://zhuanlan.zhihu.com/p/648000801'>[blog]</a><br>
</li>
</ul>


<ul><li><p>InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework<br />
    Shanglin Lei,<strong>Guanting Dong*</strong>, Xiaoping Wang, Keheng Wang, Sirui Wang<br />
    Arxiv. 
    <a href='https://arxiv.org/pdf/2309.11911.pdf'>[paper]</a>
    <a href='https://github.com/LIN-SHANG/InstructERC'>[code]</a> 
	<a href='https://mp.weixin.qq.com/s/Jj4Cf4xDmykeEYTvzNoFYg'>[blog]</a><br>
</li>
</ul>








<h2>Honors & Awards</h2>
<ul>

    <!-- <li>
        <strong>China National Scholarship</strong>, 2023
    </li> -->

    <li>
        <strong>
            <a href="http://ai.ruc.edu.cn/newslist/notice/20240301100.html">1st Place in the PhD Entrance Exam (Preliminary) at the GSAI, Renmin University of China</a>
        </strong>, 2024
    </li>

    <li>
<strong>
<a href="https://jw.beijing.gov.cn/tzgg/202410/t20241010_3916096.html">Outstanding Graduates of Beijing(Top 1%)</a>
    </strong>, 2024
    </li>
	
    <li>
        <strong>
            <a href="https://mp.weixin.qq.com/s/FhWj5PFPdgBfzp2QI8F28Q">National Scholarship for Master Students(Top 1%)</a>
        </strong>, 2023
    </li>

    <li>
        <strong>Outstanding Graduate of Master Students(Top 5%), BUPT</strong>, 2023
    </li>
    <li>
        <strong>Excellent First-class Scholarship for Master Students, BUPT(Two times)</strong>, 2021, 2022
    </li>
    <li>
        <strong>
        <a href="http://seretod.org/Challenge.html">1st Award on track 2 of SereTOD Challenge, EMNLP 2022</a>
        </strong>, 2022
    </li>
    <li>
        <strong>Gold Award for College Music Festival Instrumental Performance, Beijing</strong>, 2021
    </li>
    <li>
        <strong>The Mathematical Contest in Modeling, Honorable Mention</strong>, 2021
    </li>

</ul>


<h2>Services</h2>
<li>
<strong>PC Reviewer for</strong>: ACL, EMNLP, NAACL, COLING<br />
</li>

<li>
<strong>Reviewer for</strong>: ICLR, WWW,  ACL, EMNLP, NAACL, COLING<br />
</li>
	
<div id="footer">
	<div id="footer-text"></div>
</div>
&copy 2023 Guanting Dong  <br>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>  
ÊÄªËÆøÈóÆÈáè<span id="busuanzi_value_site_pv"></span>Ê¨° <br>
ÊÄªËÆøÂÆ¢Êï∞<span id="busuanzi_value_site_uv"></span>‰∫∫Ê¨° <br>

<a href="https://clustrmaps.com/site/1blbh"  title="Visit tracker for kunkuang.github.io"><img src="//www.clustrmaps.com/map_v2.png?d=Z8dyJa5Yjz2Z_i_LEAbfY0-TbrPurcZYl5i6ii_5Xbw&cl=ffffff" /></a>
</div>
</body>

</html>
